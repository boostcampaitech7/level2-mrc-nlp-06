{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dataset_to_df(dataset):\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\" # 넣어줄 파일  지정\n",
    "dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset_to_df(dataset[\"train\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "\n",
    "# 사용할 수 있는 구두점 정의\n",
    "PUNCTUATIONS = [\",\", \".\", \"!\", \"?\"]\n",
    "\n",
    "# 구두점을 텍스트에 적절히 삽입하는 함수\n",
    "def insert_punctuation_marks(text, answer_start, answer_length):\n",
    "    words = text.split()\n",
    "    answer_end = answer_start + answer_length\n",
    "    # 정답에 포함된 단어의 인덱스\n",
    "    qs = [i for i in range(len(words)) if answer_start <= sum(len(w) + 1 for w in words[:i]) < answer_end]\n",
    "\n",
    "    new_line = []\n",
    "    for j, word in enumerate(words):\n",
    "        # 각 단어의 시작 및 끝 인덱스 계산\n",
    "        word_start = sum(len(w) + 1 for w in words[:j])\n",
    "        word_end = word_start + len(word)\n",
    "\n",
    "        # 정답 텍스트 범위 내에는 구두점을 삽입하지 않음\n",
    "        if j not in qs and not (word_start >= answer_start and word_end <= answer_end):\n",
    "            new_line.append(PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)])\n",
    "            new_line.append(word)\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    \n",
    "    # 단어와 구두점을 하나의 문자열로 결합\n",
    "    new_line = ' '.join(new_line)\n",
    "    return new_line\n",
    "\n",
    "# AEDA를 적용하는 함수\n",
    "def apply_aeda(df, text_column, num_augs=1):\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        original_text = row[text_column]\n",
    "        # 정답의 올바른 인덱스 추출\n",
    "        answer_start = row['answers']['answer_start'][0]\n",
    "        answer_length = len(row['answers']['text'][0])\n",
    "        \n",
    "        # 원본 데이터를 증강 데이터 목록에 추가\n",
    "        augmented_data.append(row.to_dict())\n",
    "        \n",
    "        for _ in range(num_augs):\n",
    "            # 구두점 삽입을 통해 증강된 텍스트 생성\n",
    "            augmented_text = insert_punctuation_marks(original_text, answer_start, answer_length)\n",
    "            # 증강된 텍스트를 포함한 행의 복사본 생성\n",
    "            augmented_row = row.to_dict()\n",
    "            augmented_row[text_column] = augmented_text\n",
    "            # 증강된 행을 목록에 추가\n",
    "            augmented_data.append(augmented_row)\n",
    "\n",
    "    # 원본 및 증강된 데이터를 모두 포함하는 새로운 DataFrame 반환\n",
    "    return pd.DataFrame(augmented_data)\n",
    "\n",
    "# 예시 데이터프레임으로 AEDA 적용 예시\n",
    "\n",
    "train_dft = pd.DataFrame(train_df)\n",
    "\n",
    "# AEDA 적용\n",
    "df_augmented = apply_aeda(train_df, 'context', num_augs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위치를 맞추기 위해 데이터를 테스트\n",
    "df_augmented_test = df_augmented.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context와 같은 단어들의 시작 인덱스를 구하는 코드\n",
    "def get_answer_word_idx(context, answer):\n",
    "  indices = [m.start() for m in re.finditer(answer, context)]\n",
    "  return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_test[\"answer_word_idx\"] = df_augmented_test.apply(\n",
    "    lambda row: get_answer_word_idx(row[\"context\"], row[\"answers\"][\"text\"][0]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 answer_word_idx에서 answer_start와 같은 값을 가지는 인덱스 번호 저장\n",
    "def get_answer_idx(answer_start, answer_word_idx):\n",
    "  re_idx = 0\n",
    "  for idx, data in enumerate(answer_word_idx):\n",
    "    if data == answer_start:\n",
    "      re_idx = int(idx)\n",
    "\n",
    "  return re_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_test_cleaned = df_augmented_test[df_augmented_test['answer_word_idx'].apply(lambda x: len(x) > 0)] # answer_word_idx 위치를 찾지 못한 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_answer_word_idx = df_augmented_test[df_augmented_test['answer_word_idx'].apply(lambda x: len(x) == 0)] #answer_word_idx 위치를 찾지 못한 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강된 데이터만 선택 (홀수 인덱스)\n",
    "augmented_data_mask = df_augmented_test.index % 2 != 0\n",
    "df_augmented_agmented = df_augmented_test[augmented_data_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강안 된 데이터만 선택 (짝수 인덱스)\n",
    "augmented_data_mask1 = df_augmented_test.index % 2 == 0\n",
    "df_augmented_agmented33 = df_augmented_test[augmented_data_mask1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른 answer_start위치 찾기\n",
    "def align_answer_start_with_aeda(df_orig, df_augmented):\n",
    "    for index, row in df_augmented.iterrows():\n",
    "        # 원본 데이터의 answer_word_idx와 answer_start 가져오기\n",
    "        original_word_indices = df_orig.at[index, 'answer_word_idx']\n",
    "        original_answer_start = df_orig.at[index, 'answers']['answer_start'][0]\n",
    "        \n",
    "        # 증강된 데이터의 answer_word_idx 가져오기\n",
    "        augmented_word_indices = row['answer_word_idx']\n",
    "\n",
    "        if original_word_indices and augmented_word_indices:\n",
    "            # 원본 데이터에서 answer_start의 인덱스 찾기\n",
    "            original_index = original_word_indices.index(original_answer_start)\n",
    "            \n",
    "            # 증강된 데이터에서 같은 인덱스의 값 찾기\n",
    "            if original_index < len(augmented_word_indices):\n",
    "                aligned_index = augmented_word_indices[original_index]\n",
    "                \n",
    "                # 기존 answers 딕셔너리에서 answer_start 값을 업데이트\n",
    "                updated_answers = row['answers'].copy()\n",
    "                updated_answers['answer_start'] = [aligned_index]\n",
    "                df_augmented.at[index, 'answers'] = updated_answers\n",
    "                print(f\"Augmented Row {index}: Aligned answer_start to {aligned_index}\")\n",
    "            else:\n",
    "                print(f\"Augmented Row {index}: Index out of range\")\n",
    "        else:\n",
    "            print(f\"Row {index}: No valid indices found\")\n",
    "\n",
    "# 함수 실행\n",
    "align_answer_start_with_aeda(df_augmented_agmented33, df_augmented_agmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3826\n"
     ]
    }
   ],
   "source": [
    "df_augmented_cle_cleaned = df_augmented_agmented[df_augmented_agmented['answer_word_idx'].apply(lambda x: len(x) > 0)]\n",
    "print(len(df_augmented_cle_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_cle_cleaned.drop('answer_word_idx', axis=1, inplace=True)\n",
    "df_augmented_agmented33.drop('answer_word_idx', axis=1, inplace=True)\n",
    "# 필요하지 않은 컬럼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half = df_augmented_cle_cleaned.copy() # 반절의 데이터 활용을 위한 데이터 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_augmented_agmented33, df_augmented_cle_cleaned], ignore_index=True) # 나눈 데이터에 대한 병합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('/data/ephemeral/home/lji/orginal.csv', index=False) # 원본과 전체 증강 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  \n",
    "df_half = df_half.sample(frac=0.5) # 랜덤 시드를 전해준 후 값 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2 = pd.concat([df_augmented_agmented33, df_half], ignore_index=True) #원본 + 1/2 증강 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2.to_csv('/data/ephemeral/home/lji/half2.csv', index=False) # 원본 + 1/2 증강 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='/data/ephemeral/home/lji/orginal.csv')\n",
    "dataset_half = load_dataset('csv', data_files='/data/ephemeral/home/lji/half2.csv') # 허깅페이스 형태로 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'document_id', 'answers'],\n",
      "        num_rows: 7778\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'document_id', 'answers'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset,dataset_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 7778/7778 [00:00<00:00, 304327.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Arrow 포맷으로 저장\n",
    "dataset.save_to_disk('/data/ephemeral/home/lji') # 파일 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_half.save_to_disk('/data/ephemeral/home/lji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그 외 문장 확인 코드\n",
    "def verify_answer_indices(df):\n",
    "    for index, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        answer_text = row['answers']['text'][0]\n",
    "        expected_start = row['answers']['answer_start'][0]\n",
    "        word_indices = row['answer_word_idx']\n",
    "\n",
    "        # Check if the expected start index is in the list of word indices\n",
    "        if expected_start in word_indices:\n",
    "            print(f\"Row {index}: Correct - '{answer_text}' found at {expected_start}\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Incorrect - '{answer_text}' not found at expected index {expected_start}\")\n",
    "            # Optionally print the context and indices for debugging\n",
    "            print(f\"Context: {context}\")\n",
    "            print(f\"Word Indices: {word_indices}\")\n",
    "\n",
    "# Use the function on your augmented DataFrame\n",
    "verify_answer_indices(df_augmented_agmented)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
