{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Embedding - TF-IDF\n",
    "- 업스테이지 베이스라인 기반으로 작성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기\n",
    "- `wikipedia_documents.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Load JSON data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: str, path to the JSON file\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        wiki = json.load(f)\n",
    "\n",
    "    ### unique text 추출\n",
    "    wiki_df = pd.DataFrame(wiki.values())\n",
    "    wiki_unique_df = wiki_df.drop_duplicates(subset=['text'], keep='first')\n",
    "    ids = wiki_unique_df['document_id'].tolist()\n",
    "    contexts = wiki_unique_df['text'].tolist()\n",
    "    return ids, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique context: 56737\n"
     ]
    }
   ],
   "source": [
    "retrieval_data_path = '../../data/wikipedia_documents.json'\n",
    "ids, contexts = load_data(retrieval_data_path)\n",
    "print(f\"Length of unique context: {len(contexts)}\")\n",
    "# Q. 문서 제목을 활용할 수는 없을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 임베딩 생성\n",
    "- sparse embedding: TF-IDF, BM25\n",
    "- learned sparse embedding: SPLADE, BGE-M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_embedding(embedding_method, tokenize_fn):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Passage Embedding을 만들고\n",
    "        TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "        만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "    \"\"\"\n",
    "    pickle_name = f\"{embedding_method}_sparse_embedding.bin\"\n",
    "    vectorizer_name = f\"{embedding_method}.bin\"\n",
    "    emb_path = os.path.join(pickle_name)\n",
    "    vectorizer_path = os.path.join(vectorizer_name)\n",
    "\n",
    "    if os.path.isfile(emb_path) and os.path.isfile(vectorizer_path):\n",
    "        with open(emb_path, \"rb\") as file:\n",
    "            p_embedding = pickle.load(file)\n",
    "        with open(vectorizer_path, \"rb\") as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "        print(\"Embedding pickle load.\")\n",
    "    else:\n",
    "        print(\"Build passage embedding\")\n",
    "        # vectorizer 정의\n",
    "        if embedding_method == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(tokenizer=tokenize_fn.tokenize, ngram_range=(1, 2), max_features=50000)\n",
    "            p_embedding = vectorizer.fit_transform(tqdm(contexts, desc=\"TF-IDF Vectorization\"))\n",
    "        elif embedding_method == 'bm25':\n",
    "            pass ###TODO: BM25 구현\n",
    "        elif embedding_method == 'splade':\n",
    "            pass ###TODO: splade 구현\n",
    "        elif embedding_method == 'bge-m3':\n",
    "            pass ###TODO: bge-m3 구현\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported embedding method.\")\n",
    "\n",
    "        # p_embedding, vectorizer 저장\n",
    "        with open(emb_path, \"wb\") as file:\n",
    "            pickle.dump(p_embedding, file)\n",
    "        with open(vectorizer_path, \"wb\") as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "        print(\"Embedding pickle saved.\")\n",
    "    return p_embedding, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization:   0%|          | 0/56737 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "TF-IDF Vectorization: 100%|██████████| 56737/56737 [01:19<00:00, 709.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pickle saved.\n"
     ]
    }
   ],
   "source": [
    "embedding_method = 'tfidf'\n",
    "tokenize_fn = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "p_embedding, vectorizer = get_sparse_embedding(embedding_method, tokenize_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vector Database\n",
    "- FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss(num_clusters=64) -> NoReturn:\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        속성으로 저장되어 있는 Passage Embedding을\n",
    "        Faiss indexer에 fitting 시켜놓습니다.\n",
    "        이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "\n",
    "    Note:\n",
    "        Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "        매번 새롭게 build하는 것은 비효율적입니다.\n",
    "        그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "        다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "        제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    indexer_name = f\"{embedding_method}_faiss_clusters{num_clusters}.index\"\n",
    "    indexer_path = os.path.join(indexer_name)\n",
    "    if os.path.isfile(indexer_path):\n",
    "        print(\"Load Saved Faiss Indexer.\")\n",
    "        indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "    else:\n",
    "        print(f\"Creating FAISS indexer from embeddings with num_clusters {num_clusters}.\")\n",
    "        p_emb = p_embedding.astype(np.float32).toarray()\n",
    "        emb_dim = p_emb.shape[-1]\n",
    "\n",
    "        num_clusters = num_clusters\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)  ###TODO: L2 외의 다른 Metric 적용\n",
    "        indexer = faiss.IndexIVFScalarQuantizer(\n",
    "            quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "        )\n",
    "        indexer.train(p_emb)  ###TODO: 소요시간 표시 (timer 함수 이용)\n",
    "        indexer.add(p_emb)\n",
    "        faiss.write_index(indexer, indexer_path)\n",
    "        print(\"Faiss Indexer Saved.\")\n",
    "    return indexer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS indexer from embeddings with num_clusters 64.\n",
      "Faiss Indexer Saved.\n"
     ]
    }
   ],
   "source": [
    "indexer = build_faiss(num_clusters=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc(query: str, k: Optional[int] = 1, use_faiss: Optional[bool] = False) -> Tuple[List, List]:\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query (str):\n",
    "            하나의 Query를 받습니다.\n",
    "        k (Optional[int]): 1\n",
    "            상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "\n",
    "    with timer(\"transform\"):\n",
    "        query_vec = vectorizer.transform([query])\n",
    "    assert (\n",
    "        np.sum(query_vec) != 0\n",
    "    ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "    if use_faiss:\n",
    "        q_emb = query_vec.toarray().astype(np.float32)\n",
    "        with timer(\"query faiss search\"):\n",
    "            D, I = indexer.search(q_emb, k)\n",
    "        return D.tolist()[0], I.tolist()[0]\n",
    "    else:\n",
    "        with timer(\"query ex search\"):\n",
    "            result = query_vec * p_embedding.T\n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = result.toarray()\n",
    "\n",
    "        sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "        doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "        return doc_score, doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc_bulk(queries: List, k: Optional[int] = 1, use_faiss: Optional[bool] = False) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        queries (List):\n",
    "            쿼리 리스트를 받습니다.\n",
    "        k (Optional[int]): 1\n",
    "            상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "    Note:\n",
    "        vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "    \"\"\"\n",
    "\n",
    "    query_vecs = vectorizer.transform(queries)\n",
    "    assert (\n",
    "        np.sum(query_vecs) != 0\n",
    "    ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "    if use_faiss:\n",
    "        q_emb = query_vecs.toarray().astype(np.float32)\n",
    "        with timer(\"query faiss search (bulk)\"):\n",
    "            D, I = indexer.search(q_emb, k)\n",
    "        return D.tolist(), I.tolist()\n",
    "    else:\n",
    "        with timer(\"query ex search (bulk)\"):\n",
    "            result = query_vecs * p_embedding.T\n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = result.toarray()\n",
    "        doc_scores = []\n",
    "        doc_indices = []\n",
    "        for i in range(result.shape[0]):\n",
    "            sorted_result = np.argsort(result[i, :])[::-1]\n",
    "            doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "            doc_indices.append(sorted_result.tolist()[:k])\n",
    "        return doc_scores, doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(\n",
    "    query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1, use_faiss: Optional[bool] = False\n",
    ") -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        query_or_dataset (Union[str, Dataset]):\n",
    "            str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "            str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "            Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "            이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "        topk (Optional[int], optional): Defaults to 1.\n",
    "            상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "    Returns:\n",
    "        1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "        다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "\n",
    "    Note:\n",
    "        다수의 Query를 받는 경우,\n",
    "            Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "            Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_faiss:\n",
    "        assert indexer is not None, \"build_faiss() 메소드를 먼저 수행해주세요.\"\n",
    "    else:\n",
    "        assert p_embedding is not None, \"get_sparse_embedding() 메소드를 먼저 수행해주세요.\"\n",
    "\n",
    "    if isinstance(query_or_dataset, str):\n",
    "        doc_scores, doc_indices = get_relevant_doc(query_or_dataset, k=topk, use_faiss=use_faiss)  ###TODO: get_relevant_doc_faiss\n",
    "        print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "        for i in range(topk):\n",
    "            print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "            print(contexts[doc_indices[i]])\n",
    "\n",
    "        return (doc_scores, [contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "    elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "        # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "        queries = query_or_dataset[\"question\"]\n",
    "        total = []\n",
    "\n",
    "        with timer(\"query exhaustive search\"):\n",
    "            doc_scores, doc_indices = get_relevant_doc_bulk(queries, k=topk, use_faiss=use_faiss)  ###TODO: get_relevant_doc_bulk_faiss\n",
    "        \n",
    "        for idx, example in enumerate(\n",
    "            tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "        ):\n",
    "            tmp = {\n",
    "                # Query와 해당 id를 반환합니다.\n",
    "                \"question\": example[\"question\"],\n",
    "                \"id\": example[\"id\"],\n",
    "                # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                \"context\": \" \".join(\n",
    "                    [contexts[pid] for pid in doc_indices[idx]]\n",
    "                ),\n",
    "            }\n",
    "            if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                tmp[\"original_context\"] = example[\"context\"]\n",
    "                tmp[\"answers\"] = example[\"answers\"]\n",
    "            total.append(tmp)\n",
    "\n",
    "        cqas = pd.DataFrame(total)\n",
    "        return cqas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "query = \"대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\"\n",
    "\n",
    "org_dataset_path = \"../../data/train_dataset\"\n",
    "org_dataset = load_from_disk(org_dataset_path)\n",
    "\n",
    "full_ds = concatenate_datasets(\n",
    "    [\n",
    "        org_dataset[\"train\"].flatten_indices(),\n",
    "        org_dataset[\"validation\"].flatten_indices(),\n",
    "    ]\n",
    ")  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transform] done in 0.003 s\n",
      "[query faiss search] done in 0.516 s\n",
      "[Search query]\n",
      " 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은? \n",
      "\n",
      "Top-1 passage with score 3.964760\n",
      "대통령 지시(Presidential directive)는 미국 대통령의 국가안보 관련 행정명령이다. NSC의 조언과 동의(Advice and consent)가 필요하다. 국가안보 대통령 지시가 보다 뜻이 명확하다.\n",
      "\n",
      "역대 대통령들은 다양한 용어를 사용했다. 케네디 대통령은 국가안보실행메모(NSAM, National Security Action Memorandums), 닉슨과 포드 대통령은 국가안보결정메모(NSDM, National Security Decision Memorandums), 클린턴 대통령은 대통령결정지시, 조지 부시 대통령은 국가안보대통령지시, 오바마 대통령은 대통령정책지시라고 부른다.\n",
      "\n",
      "미국의 대통령 지시는 비밀명령으로 내려지기도 하는데, 이는 수십년이 지나면 비밀해제되어 일반에 공개된다. 미국 육군 정보와 보안 사령부와 같이, 군대에 특정 부대를 창설하는 경우, 대통령 지시만으로 창설되곤 한다.\n",
      "[single query by faiss] done in 0.520 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[query faiss search (bulk)] done in 329.465 s\n",
      "[query exhaustive search] done in 330.981 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 9913.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval result by faiss 0.03482824427480916\n",
      "[bulk query by exhaustive search] done in 331.418 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "use_faiss = True \n",
    "\n",
    "with timer(\"single query by faiss\"):\n",
    "    scores, indices = retrieve(query, use_faiss=use_faiss)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "with timer(\"bulk query by exhaustive search\"):\n",
    "    df = retrieve(full_ds, use_faiss=use_faiss)\n",
    "    df[\"correct\"] = df[\"original_context\"] == df[\"context\"]\n",
    "\n",
    "    print(\"correct retrieval result by faiss\", df[\"correct\"].sum() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transform] done in 0.002 s\n",
      "[query ex search] done in 0.482 s\n",
      "[Search query]\n",
      " 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은? \n",
      "\n",
      "Top-1 passage with score 0.198994\n",
      "국회에 관해 규정하는 헌법 제4장의 첫 조문이다.\n",
      "\n",
      "본조에서 말하는 \"국권\"이란 국가가 갖는 지배권을 포괄적으로 나타내는 국가 권력, 곧 국가의 통치권을 의미한다. 국권은 일반적으로 입법권·행정권·사법권의 3권으로 분류되지만, 그 중에서도 주권자인 국민의 의사를 직접 반영하는 기관으로서 국회를 \"최고 기관\"으로 규정한 것이다. 다만, 최고 기관이라 해서 타 기관의 감시와 통제를 받지 않는 것은 아니며 권력 분립 원칙에 따라 국회에 대한 행정권, 사법권의 견제를 받는다.\n",
      "\n",
      "또한 일본 전체 국민을 대표하는 기관을 국회로 규정함으로써, 국회는 일본의 유일한 입법 기관의 지위를 가지고 있다. 일본 제국 헌법 하에서 입법권은 천황의 권한에 속했으며, 제국의회는 천황의 입법 행위를 보좌하는 기관에 불과했다.\n",
      "\n",
      "여기서 \"유일한 입법 기관\"의 의미로는 다음과 같은 해석이 있다.\n",
      "* 국회 중심 입법 원칙 : 국회가 국가의 입법권을 독점한다는 원칙\n",
      "* 국회 단독 입법 원칙 : 국회의 입법은 다른 기관의 간섭 없이 이루어진다는 원칙\n",
      "\n",
      "또한 국회의 입법에 벗어나지 않는 범위 내에서 행정 기관은 정령 등의 규칙 제정권을 가지며(헌법 제73조 제6호), 최고재판소는 소송에 관한 절차, 변호사 및 재판소에 관한 내부 규율 및 사법 사무 처리에 관한 사항에 대한 규칙 제정권(헌법 제77조 제1항)을 가진다.\n",
      "[single query by exhaustive search] done in 0.490 s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[query ex search (bulk)] done in 8.709 s\n",
      "[query exhaustive search] done in 26.822 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 5495.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval result by exhaustive search 0.25166984732824427\n",
      "[bulk query by exhaustive search] done in 27.602 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "use_faiss = False \n",
    "\n",
    "with timer(\"single query by exhaustive search\"):\n",
    "    scores, indices = retrieve(query)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "with timer(\"bulk query by exhaustive search\"):\n",
    "    df = retrieve(full_ds)\n",
    "    df[\"correct\"] = df[\"original_context\"] == df[\"context\"]\n",
    "    print(\n",
    "        \"correct retrieval result by exhaustive search\",\n",
    "        df[\"correct\"].sum() / len(df),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
